{"url": "https://datascience.stackexchange.com/questions/69702/keras-oom-when-allocating-tensor-with-shape", "question_head": "Keras OOM when allocating tensor with shape", "question_body": "I have been scratching my head over this OOM Error for days and I am new to Keras. I have tried sampling down my data, lowering batch size, and removing layers from 3D-Unet but nothing is working for me. I am using LIDC IDRI dataset of CT scans of 1010 Patients. After pre-processing I save my volumes of 64x64x64 shape on disk which I extracted from resampled 256x256x256 whole CT scans (That is because at first I was first trying to train on whole CT scans but after getting OOM I decided to go with 64 cubic size shapes). Each patient has 64 shapes of 64x64x64, and in total that makes 64,640 samples on which I have to train my 3D-Unet.Here’s my Keras code for the model:There are two issues with the output I get. The first warning I get is this:It states that the shape passed to Keras library was  (64 channels), however the input shape I declared in  function of Keras is  with 1 being the channel on last axis, you don’t declare batch size here which is 8 in my case, yet Keras state that the shape passed on to it has 64 channels, ignoring the last dimension I gave it.The second error that I get is as following:Again I have a problem with shape here, Tensor Shape. My shape should be  but what it reports is , not only my number of channels are huge here but I also have no idea where that 32 came from. Is there a different interpretation to Tensor Shape? I think there’s something wrong with my input shapes (which is unknowingly being to set to very large) and that is causing the OOM error.Add a print statement before your yield statement to check that your actual input actually has a shape of (bs, 64, 64, 64, 1). Defining an  of shape (64, 64, 64, 1) just tells your model to expect input of that shape but that does not necessarily mean that your data are generated in that required shape.The OOM error could be due to having insufficient RAM. The shape of  is most probably from an intermediate layer and not the input layer. Try lowering the batch size to fix this.", "answer_body": "0$\\begingroup$Add a print statement before your yield statement to check that your actual input actually has a shape of (bs, 64, 64, 64, 1). Defining an Input() of shape (64, 64, 64, 1) just tells your model to expect input of that shape but that does not necessarily mean that your data are generated in that required shape.The OOM error could be due to having insufficient RAM. The shape of [8,32,64,64,64] is most probably from an intermediate layer and not the input layer. Try lowering the batch size to fix this.﻿ShareImprove this answerFollowanswered Mar 15 '20 at 13:08Vincent YongVincent Yong40211 silver badge66 bronze badges$\\endgroup$2$\\begingroup$Thank you for your response. I have already tried printing my input shape before yield and my input shape is indeed (8, 64, 64, 64, 1).where 8 is my batch size. Thank you for the insight on tensor shape, but it still doesn't explain the warning generated by keras library for data format convention.$\\endgroup$– Waqas AhmedMar 15 '20 at 14:33$\\begingroup$Can't seem to figure out what's wrong with your code. May I suggest an alternative? This was a custom data generator which I used for action recognition. The logic is pretty similar, just that the additional dimension is time whereas for yours, it is depth. This was the custom generator that I used: github.com/peachman05/action-recognition-tutorial/blob/master/…$\\endgroup$– Vincent YongMar 15 '20 at 15:20add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/88737/keras-next-what-does-2-256-128-128-3-mean", "question_head": "Keras next(); what does (2, 256, 128, 128, 3) mean", "question_body": "I have used the next() method on a Keras generator. I then turned this into a numpy array to get the shape:256 is my batch size, and (128, 128, 3) is my image size. But what does 2 represent?\nFirst you should unpack it with a tuple then use image and labels. - ((16, 224, 224, 3), (16, 2)).", "answer_body": "1$\\begingroup$Keras generator returns a Tuple for data and label. So is that 2.First you should unpack it with a tuple then use image and labels.yielding tuples of (x, y) where x is a numpy array containing a batch of images with shape (batch_size, *target_size, channels) and y is a numpy array of corresponding labelsimg, labels = traindata.next()img.shape, labels.shapeOutput - ((16, 224, 224, 3), (16, 2))We are also supposed to return a Tuple of batches of images/labels in case we build a custom generator for Keras.﻿ShareImprove this answerFollowanswered Jan 31 at 3:5310xAI10xAI3,51422 gold badges55 silver badges2020 bronze badges$\\endgroup$add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/88959/python-stemmer-for-georgian", "question_head": "Python stemmer for Georgian", "question_body": "I am currently working with . Does anybody know any / (or other NLP tools) for Georgian that I could use with .Thanks in advance!I don't know any Georgian stemmer or lemmatizer. I think, however, that you have another option: to use unsupervised approaches to segment words into morphemes, and use your linguistic knowledge of Georgian to devise some heuristic rules to identify the stem among them.This kind of approach consists of a model trained to identify morphemes without any labels (i.e. unsupervisedly). The most relevant Python package for this is . You can find its theoretical foundations in these publications: ; .Also, there is a Python package called  that offers pre-trained Morfessor models, . Therefore, my recommendation is for you to use Polyglot's Georgian model to segment words into morphemes and then write some rules by hand to pick the stem among them.You should be able to evaluate the feasibility of this idea by adapting  from English to Georgian (by changing the language code  and the list of words):If absolutely necessary, You could build your own stemmer. It is fairly simple programming, but takes some studying of the Georgian language in the process, there are however plenty  for building a stemming process.", "answer_body": "4$\\begingroup$I don't know any Georgian stemmer or lemmatizer. I think, however, that you have another option: to use unsupervised approaches to segment words into morphemes, and use your linguistic knowledge of Georgian to devise some heuristic rules to identify the stem among them.This kind of approach consists of a model trained to identify morphemes without any labels (i.e. unsupervisedly). The most relevant Python package for this is Morfessor. You can find its theoretical foundations in these publications: Unsupervised discovery of morphemes; Semi-supervised learning of concatenative morphology.Also, there is a Python package called Polyglot that offers pre-trained Morfessor models, including one for Georgian. Therefore, my recommendation is for you to use Polyglot's Georgian model to segment words into morphemes and then write some rules by hand to pick the stem among them.You should be able to evaluate the feasibility of this idea by adapting this example from Polyglot's documentation from English to Georgian (by changing the language code en and the list of words):from polyglot.text import Text, Wordwords = [\"preprocessing\", \"processor\", \"invaluable\", \"thankful\", \"crossed\"]for w in words:w = Word(w, language=\"en\")print(\"{:<20}{}\".format(w, w.morphemes))﻿ShareImprove this answerFollowedited 10 hours agoanswered 11 hours agonoenoe10k11 gold badge1818 silver badges4747 bronze badges$\\endgroup$2$\\begingroup$Thanks @noe for the detailed answer! I think that first of all I will take your advice and try to use Polyglot in my research.$\\endgroup$– Евгения Рубанова10 hours ago$\\begingroup$I'm glad that my answer was helpful. Please consider upvoting it. Also, please consider marking it as correct if deemed so.$\\endgroup$– noe10 hours agoadd a comment|"}
{"url": "https://datascience.stackexchange.com/questions/44438/using-image-data-along-with-csv-file-data-for-cnn-model", "question_head": "Using Image Data along with CSV file data for CNN model", "question_body": "I have image data along with csv file where each row of csv file contains attributes for corresponding image.I want to use images as well as csv file data to build CNN model using Keras.What is preferred way of doing it?I think, in 1st appraoch csv file data which has around 50 columns would hardly matter as image shape is (128*128*3). Is it correct? and how can I give more importance to csv file (like 60% weightage to image data and 40% to csv file).And again how CNN would work with that CSV file data as CNN is preferred on Image Data?Kera's  allows arbitrary models to be combined. One option is to define a \"main input\" as the image that feeds into a CNN and then define an \"auxiliary input\" that feeds into a MLP. Both of those sub-models can feed to the same higher layers. The weights of the entire model will be updated during training. The model will learn how to weight the data from csv compared to the image data (in contrast to you picking the relative weights).", "answer_body": "1$\\begingroup$Kera's functional API allows arbitrary models to be combined.One option is to define a \"main input\" as the image that feeds into a CNN and then define an \"auxiliary input\" that feeds into a MLP. Both of those sub-models can feed to the same higher layers. The weights of the entire model will be updated during training. The model will learn how to weight the data from csv compared to the image data (in contrast to you picking the relative weights).﻿ShareImprove this answerFollowanswered Jan 23 '19 at 16:23Brian SpieringBrian Spiering10.6k11 gold badge1717 silver badges5252 bronze badges$\\endgroup$add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/40679/model-predict-in-keras-python-error", "question_head": "model.predict in Keras, Python error", "question_body": "I trained a model in Keras with input dimension 15 and output dimension 1. Then I tried to predict the output for a single input np.array, which I chose to be a toy example np.arange(15). However, the input is not accepted. Can someone tell me where the problem is?\nHere is the code for a simplified problem: The following error occurs: \nValueError: Error when checking input: expected dense_4_input to have shape (15,) but got array with shape (1,). \nBut then again, the input clearly has the correct shape. What is going on here?\nThanks for your help!In Keras,  represents the number of input parameters, in your case that would be the number of columns of  or its second dimension (sometimes also referred to as number of features). It is clearly not 15. It is the first dimension that is 15. That means:  consists of 15 rows, also called samples (of one and the same feature).So in that case, . However, you will then run into the problem of having specified . First, Keras will throw an error because it is an integer. You could do , but then you will getSo you have to turn this into an array-like object containing 15 samples, e.g. a list of length 15.However, in case you meant to feed   to your model, that maps to one single output , then you need to   accordingly, for example via Can you figure out now what  needs to be?", "answer_body": "1$\\begingroup$But then again, the input clearly has the correct shape.>>> import numpy as np>>> X = np.arange(15)>>> X.shape(15,)In Keras, input_dim represents the number of input parameters, in your case that would be the number of columns of X or its second dimension (sometimes also referred to as number of features). It is clearly not 15. It is the first dimension that is 15. That means: X consists of 15 rows, also called samples (of one and the same feature).So in that case, input_dim=1.However, you will then run into the problem of having specified Y = 0. First, Keras will throw an error because it is an integer. You could do Y = [0], but then you will getValueError: Input arrays should have the same number of samples as target arrays. Found 15 input samples and 1 target samples.So you have to turn this into an array-like object containing 15 samples, e.g. a list of length 15.However, in case you meant to feed one single sample X to your model, that maps to one single output Y = [0], then you need to reshape X accordingly, for example viaX = np.arange(15).reshape(n_samples, n_features)Can you figure out now what n_samples, n_features needs to be?﻿ShareImprove this answerFollowanswered Oct 20 '18 at 6:44Andrei PoehlmannAndrei Poehlmann14611 bronze badge$\\endgroup$add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/35526/custom-loss-function-which-is-included-gradient-in-keras", "question_head": "Custom loss function which is included gradient in Keras", "question_body": "I want to make a custom loss function.\nConcretely, I use a 2D Convolutional neural network in Keras.\nSo far, I've made various custom loss function by adding to losses.py.\nHowever, in this case, I encountered the trouble which is explained later.I have attempted to make a regressor for image tasks.\nThe data shape of both input & output are (1,128,128,2), where 1 is mini-batch size, 128 is pixel number and 2 is the number of the channels.Anyway, I want to add new loss function to this task.\nI want to compute the difference of pixel gradients between answer & predicted values and add to loss function.\nI tried to make such like below.Error is written as below.Probably, in losses.py, the data type of , ... doesn't support numpy. How can I debug/fix this problem?I think it is necessary to perform  operations using the backend versions, allowing Keras to perform backpropagation on every step of the function. You use the common  for example.... try , which should work (based on the available ).Also, have a look at , where some of the mechanics around creating a custom loss function in Keras are discussed.The two main loops in your function that compute the gradients should be candidates for vecotisation, where you could compute the differences in one operation. Try something along these lines:and then the quotient in a second operation:and then of course the same for  and .@n1k31t4 Thank you for your reply and I tried your suggestion. But these errors are output as below; According to your suggested cite, tf.subtract or tf.divide are exist so I tried to rewrite as explained later and then I encountered these errors as below;Error is as below;I want to substitute 'cont_true_sum' to cont_true[i,j]. How can I fix?", "answer_body": "1$\\begingroup$I think it is necessary to perform all operations using the backend versions, allowing Keras to perform backpropagation on every step of the function. You use the common + for example.... try K.add, which should work (based on the available arithmetic operation of the tensorflow backend).Also, have a look at a related question, where some of the mechanics around creating a custom loss function in Keras are discussed.The two main loops in your function that compute the gradients should be candidates for vecotisation, where you could compute the differences in one operation. Try something along these lines:diffs = K.subtract(u_true[i+1, j], u_true[i, j])and then the quotient in a second operation:quot = K.divide(diffs, dx)and then of course the same for v_true and dy.﻿ShareImprove this answerFollowanswered Jul 16 '18 at 14:05n1k31t4n1k31t412.3k22 gold badges1717 silver badges4040 bronze badges$\\endgroup$0add a comment|1$\\begingroup$@n1k31t4Thank you for your reply and I tried your suggestion. But these errors are output as below;AttributeError: module 'keras.backend' has no attribute 'subtract'.According to your suggested cite, tf.subtract or tf.divide are exist so I tried to rewrite as explained later and then I encountered these errors as below;def continuity(y_true, y_pred):import tensorflow as tfimport numpy as npdx = dy = 1/128gridSetting = (128,128)u_true = y_true[0,:,:,0]v_true = y_true[0,:,:,1]u_pred = y_pred[0,:,:,0]v_pred = y_pred[0,:,:,1]cont_true = tf.zeros((127,127))for j in range(127):for i in range(127):diff_true_u = tf.subtract(u_true[i+1, j], u_true[i, j])cont_true_u = tf.divide(diff_true_u, dx)diff_true_v = tf.subtract(v_true[i, j+1], v_true[i, j])cont_true_v = tf.divide(diff_true_v, dy)cont_true_sum = cont_true_u + cont_true_vcont_true[i,j] = cont_true_sumError is as below;File \"DSC_multi-scale_2D.py\", line 107, in <module>autoencoder.compile(optimizer='adam', loss='continuity')File \"/home/----/anaconda3/envs/tensorflow-only/lib/python3.6/site-packages/keras/engine/training.py\", line 332, in compilesample_weight, mask)File \"/home/----/anaconda3/envs/tensorflow-only/lib/python3.6/site-packages/keras/engine/training_utils.py\", line 403, in weightedscore_array = fn(y_true, y_pred)File \"/home/----/anaconda3/envs/tensorflow-only/lib/python3.6/site-packages/keras/losses.py\", line 91, in continuitycont_true[i,j] = cont_true_sumTypeError: 'Tensor' object does not support item assignmentI want to substitute 'cont_true_sum' to cont_true[i,j]. How can I fix?﻿ShareImprove this answerFollowanswered Jul 17 '18 at 1:16kainamanamakainamanama12111 gold badge22 silver badges77 bronze badges$\\endgroup$add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/33145/keras-exception-valueerror-error-when-checking-input-expected-conv2d-1-input", "question_head": "Keras exception: ValueError: Error when checking input: expected conv2d_1_input to have shape (150, 150, 3) but got array with shape (256, 256, 3)", "question_body": "I am working on multiclass classification of images. For this I created a CNN model in keras. I already pre-processed all images to size (150,150,3). Here is model summary-I am also using data augmentation and  method-Then I compile the model and run -At this part I get error-I don't understand when all input images have size (150, 150, 3), how can it get (256, 256, 3)? Please tell me where I am going wrong.The code with which I created model is-For image preprocessing, I used following-\nYour problem is definitely in the generators, in that you do not set the target size, and its default it (256, 256) - as seen in :Try setting the target size parameter to (150, 150) and I think it will work. That default seems to be overwriting your preprocessing.It must be in your generators - I ran the following code and a model trained as expected:", "answer_body": "4$\\begingroup$[EDIT:]Your problem is definitely in the generators, in that you do not set the target size, and its default it (256, 256) - as seen in the documentation fro flow_from_directory:flow_from_directory(directory, target_size=(256, 256), color_mode='rgb', ...)target_size: Tuple of integers (height, width), default: (256, 256). The dimensions to which all images found will be resized.Try setting the target size parameter to (150, 150) and I think it will work. That default seems to be overwriting your preprocessing.It must be in your generators - I ran the following code and a model trained as expected:from keras import models, layers, metricsimport numpy as npmodel = models.Sequential()...: model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(150, 150,...:  3)))...: model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))...: model.add(layers.Conv2D(64, (3, 3), activation='relu'))...: model.add(layers.MaxPooling2D((2, 2)))...: model.add(layers.Conv2D(64, (3, 3), activation='relu'))...: model.add(layers.MaxPooling2D((2, 2)))...: model.add(layers.Conv2D(128, (3, 3), activation='relu'))...: model.add(layers.MaxPooling2D((2, 2)))...: model.add(layers.Flatten())...: model.add(layers.Dense(300, activation='relu'))...: model.add(layers.Dense(10, activation='softmax'))In [10]: model.summary()_________________________________________________________________Layer (type)                 Output Shape              Param #=================================================================conv2d_1 (Conv2D)            (None, 146, 146, 32)      2432_________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32)        0_________________________________________________________________conv2d_2 (Conv2D)            (None, 71, 71, 64)        18496_________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64)        0_________________________________________________________________conv2d_3 (Conv2D)            (None, 33, 33, 64)        36928_________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0_________________________________________________________________conv2d_4 (Conv2D)            (None, 14, 14, 128)       73856_________________________________________________________________max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0_________________________________________________________________flatten_1 (Flatten)          (None, 6272)              0_________________________________________________________________dense_1 (Dense)              (None, 300)               1881900_________________________________________________________________dense_2 (Dense)              (None, 10)                3010=================================================================Total params: 2,016,622Trainable params: 2,016,622Non-trainable params: 0_________________________________________________________________In [17]: model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics = ['...: acc',metrics.categorical_accuracy])# Create some fake data to match your inputs. Each label seems to be 10 points: (1, 10)In [11]: fakes = np.random.randint(0, 255, (100, 150, 150, 3))In [24]: labels = np.random.randint(0, 2, (100, 10))In [25]: model.fit(fakes, labels, validation_split=0.2)Epoch 1/1080/80 [==============================] - 2s - loss: 62.8913 - acc: 0.1125 - categorical_accuracy: 0.1125 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00Epoch 2/1080/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00Epoch 3/1080/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00Epoch 4/1080/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00Epoch 5/1080/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00Epoch 6/1080/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00Epoch 7/1080/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00Epoch 8/1080/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00Epoch 9/1080/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00Epoch 10/1080/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00﻿ShareImprove this answerFollowedited Jun 14 '18 at 11:49answered Jun 14 '18 at 11:48n1k31t4n1k31t412.3k22 gold badges1717 silver badges4040 bronze badges$\\endgroup$1$\\begingroup$Yeah, I just tried, it started to run smoothly. I thought it takes image size by default. Thanks for help.$\\endgroup$– Ankit SethJun 14 '18 at 11:49add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/21887/keras-input-dimension-bug", "question_head": "Keras input dimension bug?", "question_body": "Keras has a problem with the input dimension. My first layer looks like this:As you can see the input dimension should be (150,) and with the fixed batch_size it is (1, 150)My data has dimension (150,) and could be for example a numpy array with 150 zeros.Here I call the model to make a prediction. Normally Keras should automatically add the batch size as an extra dimension so I should end up with (1, 150) which would work. But Keras adds the dimension for the batch size at the wrong place and I end up with (150, 1). I tried tensorflow and theano backend. Do I have a bug in my code or is it a problem with Keras?How can I fix the problem? I could reshape my input data but it already has the needed shape of (150,) and should be fine. What else could I do?If I should provide more data or code feel free to ask.When you do  the first axis in  is always an index in a batch. So if you want to predict on one sample, do something like In your case, this should work:", "answer_body": "2$\\begingroup$When you do model.predict(X) the first axis in X is always an index in a batch. So if you want to predict on one sample, do something like X = np.expand_dims(X, axis=0)In your case, this should work:old_qval = model.predict( np.expand_dims(old_state_m, axis=0) )﻿ShareImprove this answerFollowedited Aug 2 '17 at 12:17community wiki2 revsNeil Slater$\\endgroup$2$\\begingroup$I have made this a community Wiki, since it is just a copy of Mikhail Yurasov's comment.$\\endgroup$– Neil SlaterAug 2 '17 at 11:40$\\begingroup$Thank you and very nice of you not to steal others credit props for you$\\endgroup$– IlovescienceAug 2 '17 at 12:44add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/88737/keras-next-what-does-2-256-128-128-3-mean?answertab=votes", "question_head": "Keras next(); what does (2, 256, 128, 128, 3) mean", "question_body": "I have used the next() method on a Keras generator. I then turned this into a numpy array to get the shape:256 is my batch size, and (128, 128, 3) is my image size. But what does 2 represent?\nFirst you should unpack it with a tuple then use image and labels. - ((16, 224, 224, 3), (16, 2)).", "answer_body": "1$\\begingroup$Keras generator returns a Tuple for data and label. So is that 2.First you should unpack it with a tuple then use image and labels.yielding tuples of (x, y) where x is a numpy array containing a batch of images with shape (batch_size, *target_size, channels) and y is a numpy array of corresponding labelsimg, labels = traindata.next()img.shape, labels.shapeOutput - ((16, 224, 224, 3), (16, 2))We are also supposed to return a Tuple of batches of images/labels in case we build a custom generator for Keras.﻿ShareImprove this answerFollowanswered Jan 31 at 3:5310xAI10xAI3,51422 gold badges55 silver badges2020 bronze badges$\\endgroup$add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/88737/keras-next-what-does-2-256-128-128-3-mean?answertab=oldest", "question_head": "Keras next(); what does (2, 256, 128, 128, 3) mean", "question_body": "I have used the next() method on a Keras generator. I then turned this into a numpy array to get the shape:256 is my batch size, and (128, 128, 3) is my image size. But what does 2 represent?\nFirst you should unpack it with a tuple then use image and labels. - ((16, 224, 224, 3), (16, 2)).", "answer_body": "1$\\begingroup$Keras generator returns a Tuple for data and label. So is that 2.First you should unpack it with a tuple then use image and labels.yielding tuples of (x, y) where x is a numpy array containing a batch of images with shape (batch_size, *target_size, channels) and y is a numpy array of corresponding labelsimg, labels = traindata.next()img.shape, labels.shapeOutput - ((16, 224, 224, 3), (16, 2))We are also supposed to return a Tuple of batches of images/labels in case we build a custom generator for Keras.﻿ShareImprove this answerFollowanswered Jan 31 at 3:5310xAI10xAI3,51422 gold badges55 silver badges2020 bronze badges$\\endgroup$add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/88737/keras-next-what-does-2-256-128-128-3-mean?answertab=active", "question_head": "Keras next(); what does (2, 256, 128, 128, 3) mean", "question_body": "I have used the next() method on a Keras generator. I then turned this into a numpy array to get the shape:256 is my batch size, and (128, 128, 3) is my image size. But what does 2 represent?\nFirst you should unpack it with a tuple then use image and labels. - ((16, 224, 224, 3), (16, 2)).", "answer_body": "1$\\begingroup$Keras generator returns a Tuple for data and label. So is that 2.First you should unpack it with a tuple then use image and labels.yielding tuples of (x, y) where x is a numpy array containing a batch of images with shape (batch_size, *target_size, channels) and y is a numpy array of corresponding labelsimg, labels = traindata.next()img.shape, labels.shapeOutput - ((16, 224, 224, 3), (16, 2))We are also supposed to return a Tuple of batches of images/labels in case we build a custom generator for Keras.﻿ShareImprove this answerFollowanswered Jan 31 at 3:5310xAI10xAI3,51422 gold badges55 silver badges2020 bronze badges$\\endgroup$add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/tagged/image-preprocessing", "question_head": null, "question_body": "", "answer_body": ""}
{"url": "https://datascience.stackexchange.com/questions/tagged/keras", "question_head": null, "question_body": "", "answer_body": ""}
{"url": "https://datascience.stackexchange.com/questions/ask", "question_head": null, "question_body": "", "answer_body": ""}
{"url": "https://datascience.stackexchange.com/questions", "question_head": null, "question_body": "", "answer_body": ""}
{"url": "https://datascience.stackexchange.com/questions/84652/stemmer-or-dictionary", "question_head": "Stemmer or dictionary?", "question_body": "I have recently ported a stemmer from Java to Python for a highly inflectional language.The stemmer learns how to change suffixes from the dictionary of words and their inflected forms. It basically builds a stemming table with learned stemming rules. As I was porting the algorithm I decided to train it on a larger dictionary. As a result, the learned stemming table got bigger, and stemming accuracy got higher as well.Then I thought this actually make no sense as the stemming table size gets closer and closer to the size of the dictionary.Why build or train stemming algorithms if you can simply lookup a dictionary?I can understand that in old times storing large files could be a problem, but now? And for some languages there might be no proper dictionary resources. But is there any other reason?There is another reason: words which don't appear in the dictionary. Of course a dictionary approach will correctly stem all the forms which are known in the dictionary, and depending on the language this may indeed lead to better accuracy. However the dictionary approach cannot do anything about unknown words, whereas a generic stemmer can try to apply its generic rules. This can be particularly important with texts which are either very domain-specific (e.g. medicine), which often contain technical words which are not in a general dictionary, or recent user-generated texts such as social media posts where people may use neologisms or words borrowed and sometimes transformed from another language.", "answer_body": "3$\\begingroup$There is another reason: words which don't appear in the dictionary. Of course a dictionary approach will correctly stem all the forms which are known in the dictionary, and depending on the language this may indeed lead to better accuracy. However the dictionary approach cannot do anything about unknown words, whereas a generic stemmer can try to apply its generic rules. This can be particularly important with texts which are either very domain-specific (e.g. medicine), which often contain technical words which are not in a general dictionary, or recent user-generated texts such as social media posts where people may use neologisms or words borrowed and sometimes transformed from another language.﻿ShareImprove this answerFollowanswered Oct 29 '20 at 17:50ErwanErwan12.1k33 gold badges77 silver badges2222 bronze badges$\\endgroup$0add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/80033/how-does-tf-idf-classify-a-document-based-on-score-alloted-to-each-word", "question_head": "How does TF-IDF classify a document based on “Score” alloted to each word", "question_body": "I understand how TF-IDF \"score\" is calculated for each word in a document, but I do not get how can it be used to classify a test document. For example, if the word \"Mobile\" occurs in two texts, in the training data, one about Business (like the selling of Mobiles) and the other about Tech, then how does the \"score\" for word \"Mobile\", in both training and test document over the given dataset, help the algorithm to classify whether the text (a new test document) belongs to \"Business\" category or \"Tech\" category? I'm new to NLP, thanks in advance!It's not a single TFIDF score on its own which makes classification possible, the TFIDF scores are used inside a vector to represent a full document: for every single word  in the vocabulary, the th value in the vector contains the corresponding TFIDF score. By using this representation for every document in a collection (the same index always corresponds to the same word), one obtains a big set of vectors (instances), each containing  TFIDF scores (features).Assuming we have some training data (labelled documents), we can use any supervised method to learn a model, for instance Naive Bayes, Decision Trees, SVM, etc. These algorithms differ from each other but they are all able to take into account  for a document in order to predict a label. So in the example you give maybe the word \"mobile\" only helps the algorithm eliminate the categories \"sports\" and \"literature\", but maybe some other words (or absence of other words) is going to help the algorithm decide between categories \"Business\" and \"Tech\".", "answer_body": "0$\\begingroup$It's not a single TFIDF score on its own which makes classification possible, the TFIDF scores are used inside a vector to represent a full document: for every single word $w_i$ in the vocabulary, the $i$th value in the vector contains the corresponding TFIDF score. By using this representation for every document in a collection (the same index always corresponds to the same word), one obtains a big set of vectors (instances), each containing $N$ TFIDF scores (features).Assuming we have some training data (labelled documents), we can use any supervised method to learn a model, for instance Naive Bayes, Decision Trees, SVM, etc. These algorithms differ from each other but they are all able to take into account all the features for a document in order to predict a label. So in the example you give maybe the word \"mobile\" only helps the algorithm eliminate the categories \"sports\" and \"literature\", but maybe some other words (or absence of other words) is going to help the algorithm decide between categories \"Business\" and \"Tech\".﻿ShareImprove this answerFollowanswered Aug 10 '20 at 0:06ErwanErwan12.1k33 gold badges77 silver badges2222 bronze badges$\\endgroup$2$\\begingroup$So we use TFIDF score for only for feature extraction  and some other algorithm does the job of classification?$\\endgroup$– hakiki_makatoAug 10 '20 at 9:55$\\begingroup$your question is a bit confusing: the TFIDF score itself is used as a feature by the algorithm, it needs some features to work with. But yes, there must be an algorithm, TFIDF by itself cannot do any classification.$\\endgroup$– ErwanAug 10 '20 at 14:42add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/42107/mixed-effect-random-forest-model-for-python-windows", "question_head": "Mixed effect random forest model for Python Windows", "question_body": "Does anybody know if there is a Mixed effect random forest model for Python Windows? The merf package  seems to only be available on a linux environment? thanks!I have tried installing \"merf\" on Windows 10 - it worked. I did it like this:", "answer_body": "0$\\begingroup$I have tried installing \"merf\" on Windows 10 - it worked. I did it like this:pip install merf﻿ShareImprove this answerFollowanswered Dec 4 '18 at 12:51Yaroslaw HomenkoYaroslaw Homenko39111 silver badge1010 bronze badges$\\endgroup$add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/26564/classify-phrases-as-biomedical-or-non-biomedical", "question_head": "Classify phrases as biomedical or non-biomedical", "question_body": "Words like  are biomedical, but words like  appear in regular English texts. I can imagine a few ways to classify words into biomedical and non-biomedical. Has this problem been tackled before? Does anybody know any ready-to-use solutions for this problem?Yes. There has been work going on in the healthcare domain which involves NLP.Resources like  have been put together for the exact same purpose. You can make use of the freely available  for training your models like maybe a sentence tagging style model for identification. We(my team at work) use them for identification of medical terms like diseases, medicines, etc.", "answer_body": "1$\\begingroup$Yes. There has been work going on in the healthcare domain which involves NLP.Resources like Pubmed have been put together for the exact same purpose.You can make use of the freely available Pubmed vectors for training your models like maybe a sentence tagging style model for identification. We(my team at work) use them for identification of medical terms like diseases, medicines, etc.﻿ShareImprove this answerFollowanswered Jan 12 '18 at 16:11Dawny33♦Dawny337,5561111 gold badges4141 silver badges100100 bronze badges$\\endgroup$4$\\begingroup$I don't need advanced identification of disease names or medicines. I only need to determine if a phrase is biomedical or not. Is there a ready to use tool? Could you show an example on how use bio.nlplab.org to determine whether a phrase is biomedical or not?$\\endgroup$– user1424739Jan 13 '18 at 7:19$\\begingroup$@user1424739 You just need to check if the word exists in the Pubmed vector or not.$\\endgroup$– Dawny33♦Jan 13 '18 at 9:43$\\begingroup$But a word that appears in Pubmed vector does not mean the word is biomedical.$\\endgroup$– user1424739Jan 13 '18 at 14:30$\\begingroup$No. But, you would be close.  This is the only research I found related to this topic.$\\endgroup$– Dawny33♦Jan 13 '18 at 16:05add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/17294/nlp-what-are-some-popular-packages-for-multi-word-tokenization", "question_head": "NLP: What are some popular packages for multi-word tokenization?", "question_body": "I intend to tokenize a number of job description texts. I have tried the standard tokenization using whitespace as the delimiter. However I noticed that there are some multi-word expressions that are splitted by whitespace, which may well cause accuracy problems in subsequent processing. So I want to get all the most interesting/informative collocations in these texts. For example, \"He studies Information Technology\" ===> \"He\" \"studies\" \"Information Technology\".I've noticed NLTK (Python) has some related functionalities. The MWETokenizer class in the nltk.tokenize.mwe module seems working towards my objective. However MWETokenizer seems to require me to use its construction method and .add_mwe method to add multi-word expressions. Thanks!The multiword tokenizer 'nltk.tokenize.mwe' basically merges a string already divided into tokens, based on a lexicon, from what I understood from the API documentation.One thing you can do is tokenize and tag all words with it's associated part-of-speech (PoS) tag, and then define regular expressions based on the PoS-tags to extract interesting key-phrases. For instance, an example adapted from the  and  :You defined a grammar based on regex over PoS-tags:Then you applied it to a tokenized and tagged sentence, generating a Tree:Then you use  to recursively parse the Tree and extract sub-trees labeled as NP. The example above would extract the following noun-phrases:There is a great blog post by Burton DeWilde about many more ways to extract interesting keyphrases: The tokenization process shouldn't be changed even when you are interested in multi words. After all, the words are still the basic tokens. What you should do it to find a way to combine the proper words into term.A simple way to do so is to look for term in which the probability of the term is higher than that of the independent tokens. \nFor example P(\"White house\") > P(\"White\")*P(\"House\")\nChoosing the proper values of need lift, number of occurrences and term classification can be deduce if you have a dataset of terms form the domain.\nIf you don't have such a domain then requirement at least 10 occurrences and and a lift of at least 2 (usually it is much higher since each token probability is low) will work quite well.In your case can can also extract terms by combining contexts relevant to your domain (e.g., \"studied X\", \"practiced Y\").Again, you can build complex and elegant models for that but usually, looking for the few next words after the context indicators will be very beneficial.For your problem i think gensim can be very useful, what can be implemented with Gensim library is phrase detection. It is similar to n-gram, but instead of getting all the n-gram by sliding the window, it detects frequently used phrases and stick them together. It statistically walks through the text corpus and identifies the common side-by-side occuring words.\nFollowing is the way it calculates the best suitable multi word tokens.\nFollowing is the code to use it. it calculates the two word tokens.and this is how you would use itNotice the word \"new_york\" which is concatenated, since in the corpus, statistical evidence of both \"new\" and \"york\" words coming together was significant. Moreover, you can go upto n-grams for this not just bi-grams.\nHere is the  which explains it in detail.This  to capture MultiWord Expressions(MWE's) worked like a charm for one of my tasks. For Python users, gear up to write some connector code or hack the Java code. gensim is one of the best to do nlp tasks on text data, \nUse  library for multi word tokenization. I found it when I was working on similar task and it worked pretty well!Updated: You can use Stanford  CoreNLP pipeline which includes multi word tokenization model. Link of demo for training neural networks with your own data is ", "answer_body": "2$\\begingroup$The multiword tokenizer 'nltk.tokenize.mwe' basically merges a string already divided into tokens, based on a lexicon, from what I understood from the API documentation.One thing you can do is tokenize and tag all words with it's associated part-of-speech (PoS) tag, and then define regular expressions based on the PoS-tags to extract interesting key-phrases.For instance, an example adapted from the NLTK Book Chapter 7 and  this blog post:def extract_phrases(my_tree, phrase):my_phrases = []if my_tree.label() == phrase:my_phrases.append(my_tree.copy(True))for child in my_tree:if type(child) is nltk.Tree:list_of_phrases = extract_phrases(child, phrase)if len(list_of_phrases) > 0:my_phrases.extend(list_of_phrases)return my_phrasesdef main():sentences = [\"The little yellow dog barked at the cat\",\"He studies Information Technology\"]grammar = \"NP: {<DT>?<JJ>*<NN>|<NNP>*}\"cp = nltk.RegexpParser(grammar)for x in sentences:sentence = pos_tag(tokenize.word_tokenize(x))tree = cp.parse(sentence)print \"\\nNoun phrases:\"list_of_noun_phrases = extract_phrases(tree, 'NP')for phrase in list_of_noun_phrases:print phrase, \"_\".join([x[0] for x in phrase.leaves()])You defined a grammar based on regex over PoS-tags:grammar = \"NP: {<DT>?<JJ>*<NN>|<NNP>*}\"cp = nltk.RegexpParser(grammar)Then you applied it to a tokenized and tagged sentence, generating a Tree:sentence = pos_tag(tokenize.word_tokenize(x))tree = cp.parse(sentence)Then you use extract_phrases(my_tree, phrase) to recursively parse the Tree and extract sub-trees labeled as NP. The example above would extract the following noun-phrases:Noun phrases:(NP The/DT little/JJ yellow/JJ dog/NN) The_little_yellow_dog(NP the/DT cat/NN) the_catNoun phrases:(NP Information/NNP Technology/NNP) Information_TechnologyThere is a great blog post by Burton DeWilde about many more ways to extract interesting keyphrases: Intro to Automatic Keyphrase Extraction﻿ShareImprove this answerFollowedited Feb 7 '19 at 10:08Noman544 bronze badgesanswered Apr 16 '17 at 2:41David BatistaDavid Batista1331111 bronze badges$\\endgroup$add a comment|1$\\begingroup$The tokenization process shouldn't be changed even when you are interested in multi words. After all, the words are still the basic tokens. What you should do it to find a way to combine the proper words into term.A simple way to do so is to look for term in which the probability of the term is higher than that of the independent tokens.For example P(\"White house\") > P(\"White\")*P(\"House\")Choosing the proper values of need lift, number of occurrences and term classification can be deduce if you have a dataset of terms form the domain.If you don't have such a domain then requirement at least 10 occurrences and and a lift of at least 2 (usually it is much higher since each token probability is low) will work quite well.In your case can can also extract terms by combining contexts relevant to your domain (e.g., \"studied X\", \"practiced Y\").Again, you can build complex and elegant models for that but usually, looking for the few next words after the context indicators will be very beneficial.﻿ShareImprove this answerFollowanswered Mar 2 '17 at 11:06DaLDaL2,46399 silver badges1313 bronze badges$\\endgroup$add a comment|1$\\begingroup$For your problem i think gensim can be very useful, what can be implemented with Gensim library is phrase detection. It is similar to n-gram, but instead of getting all the n-gram by sliding the window, it detects frequently used phrases and stick them together. It statistically walks through the text corpus and identifies the common side-by-side occuring words.Following is the way it calculates the best suitable multi word tokens.Following is the code to use it. it calculates the two word tokens.from gensim.models.phrases import Phrases, Phrasertokenized_train = [t.split() for t in x_train]phrases = Phrases(tokenized_train)bigram = Phraser(phrases)and this is how you would use itNotice the word \"new_york\" which is concatenated, since in the corpus, statistical evidence of both \"new\" and \"york\" words coming together was significant.Moreover, you can go upto n-grams for this not just bi-grams.Here is the article which explains it in detail.﻿ShareImprove this answerFollowanswered Feb 8 '19 at 15:52Qaisar RajputQaisar Rajput11144 bronze badges$\\endgroup$add a comment|0$\\begingroup$This extension of Stanford CoreNLP to capture MultiWord Expressions(MWE's) worked like a charm for one of my tasks. For Python users, gear up to write some connector code or hack the Java code.﻿ShareImprove this answerFollowanswered Sep 27 '17 at 16:40yottabyttyottabytt11122 bronze badges$\\endgroup$add a comment|0$\\begingroup$gensim is one of the best to do nlp tasks on text data,Gensim python library﻿ShareImprove this answerFollowanswered Feb 7 '19 at 6:25JAbrJAbr18111 silver badge22 bronze badges$\\endgroup$add a comment|0$\\begingroup$Use Stanford CoreNLP library for multi word tokenization. I found it when I was working on similar task and it worked pretty well!Updated: You can use Stanford  CoreNLP pipeline which includes multi word tokenization model. Link of demo for training neural networks with your own data is here﻿ShareImprove this answerFollowedited Feb 8 '19 at 12:15answered Feb 7 '19 at 6:13NomanNoman544 bronze badges$\\endgroup$4$\\begingroup$Maybe you could add also a small snippet of code to let the OP knows how to use it.$\\endgroup$– TasosFeb 7 '19 at 11:06$\\begingroup$Yes, but detailed information on how to use it and train with own data is given in the link mentioned. I also added a specific link which gives demo for training their model for multi-word tokenization$\\endgroup$– NomanFeb 8 '19 at 12:11$\\begingroup$It's a common thing that link-on answers should be avoided for various reasons. For example, the content of the page might change or the link will not be reachable in the future. When a new user visits this page, should be able to get the information the OP asked for. Just as best-practice.$\\endgroup$– TasosFeb 8 '19 at 12:14$\\begingroup$Oh now I get it, thanks for pointing out. I will try to find my code for using CoreNLP mwt model or I will code it again and paste it here (in couple of days) for the sake of information of community!$\\endgroup$– NomanFeb 8 '19 at 12:19add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/62698/how-to-feed-data-to-multi-output-keras-model-from-a-single-tfrecords-file", "question_head": "How to feed data to multi-output Keras model from a single TFRecords file", "question_body": "I know how to feed data to a  using numpy arrays for the training data. However, I have all my data in a single  comprising several feature columns: an image, which is used as input to the Keras model, plus a sequence of outputs corresponding to different classification tasks: eg. an output encodes the age of the person in the image, another output encodes the gende, and so on. From what I have seen in examples, when the output of the model is made of various heads, the model should be fed with multiple data sources, one for the input, and one for each of the ouputs. Is there an easy way to do that when the data is all in a single TFRecords? I mean, without having to create separate TFRecords for the input and each of the ouputs?After playing around with  operations I found the answer was easier than expected, I simply had to preprocess the data and put all the labels for each output of the model as a different key of a dictionary. First I create a dataset from the tfrecords fileNext, I parse data from the fileAt this point, we have a standard dataset we can operate with doing batching, shuffling, augmentation or whatever we wan. Finally, before feeding the data into the model, we have to transform it to fit the requirements of the model. The code below shows an example of both input and label preprocessing. Previoulsy, I concatenated all the labels, now I create a dictionary witht the names of the outputs in the model as keys.In my model, Gender, Ethnicity and Age are the names of the last layers of the model, so my model is defined as having three outputs: Now I can use a dataset to fit the model by applying the preprocessing function first:Considering your model recevie an  as input and has two outputs  and , and that you have generated a TFRecord with them. You can decode and use your TFRecord through  this way:", "answer_body": "1$\\begingroup$After playing around with tf.data.map operations I found the answer was easier than expected, I simply had to preprocess the data and put all the labels for each output of the model as a different key of a dictionary.First I create a dataset from the tfrecords filedataset = tf.data.TFRecordDataset(tfrecords_file)Next, I parse data from the filefeature = {'image/encoded': tf.io.FixedLenFeature((), tf.string),'image/shape': tf.io.FixedLenFeature((3), tf.int64),'age': tf.io.FixedLenFeature((), tf.int64),'gender': tf.io.FixedLenFeature((), tf.int64),'ethnicity': tf.io.FixedLenFeature((), tf.int64),}return tf_util.parse_pb_message(protobuff_message, feature)dataset = dataset.map(parser).map(process_example)At this point, we have a standard dataset we can operate with doing batching, shuffling, augmentation or whatever we wan. Finally, before feeding the data into the model, we have to transform it to fit the requirements of the model. The code below shows an example of both input and label preprocessing. Previoulsy, I concatenated all the labels, now I create a dictionary witht the names of the outputs in the model as keys.def preprocess_input_fn():def _preprocess_input(image,image_shape, age, gender, ethnicity):image = self.preprocess_image(image)labels = self.preprocess_labels(age, gender, ethnicity)return image, labelsreturn _preprocess_inputdef preprocess_image(image):image = tf.cast(image)image = tf.image.resize(image)image = (image / 127.5) - 1.0return imagedef preprocess_labels(age,gender,ethnicity):gender = tf.one_hot(gender, 2)ethnicity = tf.one_hot(ethnicity, self.ethnic_groups)age = tf.one_hot(age, self.age_groups)return {'Gender': gender, 'Ethnicity': ethnicity, 'Age': age}In my model, Gender, Ethnicity and Age are the names of the last layers of the model, so my model is defined as having three outputs:model = Model(inputs=inputs,outputs=[gender, ethnic_group, age_group])Now I can use a dataset to fit the model by applying the preprocessing function first:data = dataset.map(preprocess_input_fn())model.fit(data, epochs=...)﻿ShareImprove this answerFollowanswered Nov 28 '19 at 14:33magomarmagomar11144 bronze badges$\\endgroup$1$\\begingroup$I do not have comment rights on this exchange site yet but I can't find tf_util.parse_pb_message(protobuff_message, feature) in any tensorflow reference material. Is this a self defined function or a tensorflow method?$\\endgroup$– theastronomistJun 24 '20 at 15:53add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/72553/keras-value-error", "question_head": "Keras Value Error", "question_body": "Not new to Python or programming but I'm fairly new to machine learning so I was hoping I'm just overlooking something simple. At the moment I keep running into an issue where Keras spits out the following Value Error:So to provide some context and show you all the bit of code I'm working with, please refer to the example below.Though it may be obvious, the \"model.fit\" line is the one causing the actual error but I suspect something is wrong with the way I am bringing in data? I'm not sure if I need to reshape my array somehow or if the problem is stemming from something even more fundamental? Any help would be appreciated.Thanks,\n-BIn the code of your model, you have split up the dataset into 85% training and 15% testing dataset. However, model.fit() takes as input the features and the target variable. In your case, you have not defined the target variable for either the training or the test case. Also, as pointed out by @chappers in the comment section, you don't have to specify the batch size in the input_size.\nThe correct way to do it is as follows:", "answer_body": "1$\\begingroup$In the code of your model, you have split up the dataset into 85% training and 15% testing dataset. However, model.fit() takes as input the features and the target variable. In your case, you have not defined the target variable for either the training or the test case. Also, as pointed out by @chappers in the comment section, you don't have to specify the batch size in the input_size.The correct way to do it is as follows:# for the sake of argument, let's say we are feeding in an array of floats labeled 'getReturns'# in this case, lets say 'getReturns' contains a total of 569 values# in this case, consider 'getReturnsLabels' contains a total of 569 values with each value representing a specific class.trainingLen = int(float(len(getReturns)) * 0.85) # reserves 85 percent for trainingtrainingData = getReturns[:trainingLen]testingData = getReturns[trainingLen:-1]trainingDataLabels = getReturnsLabels[:trainingLen]testingDataLabels = getReturnsLabels[trainingLen:-1]model = keras.Sequential()model.add(keras.layers.Flatten(input_shape = (1)))model.add(keras.layers.Dense(59, activation = \"relu\"))model.add(keras.layers.Dense(1, activation = \"softmax\"))model.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])model.fit(trainingData, trainingDataLabels, epochs = 5)﻿ShareImprove this answerFollowanswered Apr 19 '20 at 0:00rajdeep sarkarrajdeep sarkar2111 bronze badge$\\endgroup$1$\\begingroup$Thanks very much for the response, Rajdeep! I can see where I went wrong now. However, over the past couple days I've been trying to learn why we need training / testing labels in the first place. So forgive the awkward question, but to clarify is this an example of Supervised Learning (therefore we require labels)?$\\endgroup$– bg07Apr 26 '20 at 12:30add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/68281/getting-typeerror-expected-bytes-descriptor-found-while-importing-tensorflow", "question_head": "Getting TypeError: expected bytes, Descriptor found while importing tensorflow", "question_body": "I am trying to use tensorflow and keras for one of the tasks but while importing the tensorflow I am getting the below error. Till now I have removed the virtual which I have created previously and then created the new virtual environment again, but still I am getting the same error. Any leads would be appreciated. Here is the command below which I am running.Here is the error I am gettingFile \"C:\\Users\\Lenovo\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow__init__.py\", line 28, in \n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-importBelow is the configuration of my system.I resolved the issue by creating new environment with tensorflow by the below two commands:Try this, worked for me:", "answer_body": "0$\\begingroup$I resolved the issue by creating new environment with tensorflow by the below two commands:conda create -n tensorflow_env tensorflowconda activate tensorflow_env﻿ShareImprove this answerFollowanswered Feb 19 '20 at 7:30manpreet singhmanpreet singh1144 bronze badges$\\endgroup$add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/71630/valueerror-error-when-checking-input-expected-conv2d-25-input-to-have-shape-1", "question_head": "ValueError: Error when checking input: expected conv2d_25_input to have shape (144, 256, 3) but got array with shape (256, 144, 3)", "question_body": "I have built a CNN with images of size 720 (height) and 1280 (width). I attempt to re scale images to 144x256. However, I receive this error:I do the obvious thing and swap the height and width, but the error still persists:The error is the same but reversed. Here is my code, ran on google colab. I have embedded the data from github into the project so you should have no problem running the entire program. Can someone explain where the error originates from? Thank you!Clearly, there is this inconsistency between the shape of input images:and the input shape of the model:These two should be the same. So, in order to save effort and make the smallest changes, just fix the input shape of the model as follow:", "answer_body": "2$\\begingroup$Clearly, there is this inconsistency between the shape of input images:target_size = (image_width, image_height)and the input shape of the model:input_shape = (image_height, image_width, 3)These two should be the same. So, in order to save effort and make the smallest changes, just fix the input shape of the model as follow:input_shape = (image_width, image_height, 3)﻿ShareImprove this answerFollowanswered Apr 10 '20 at 18:53todaytoday33422 silver badges77 bronze badges$\\endgroup$add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/86506/number-of-units-for-first-layer-in-keras-sequential-model", "question_head": "Number of units for first layer in Keras Sequential Model", "question_body": "I have a huge CSV structured dataset. I'm feeding that dataset to a Keras Sequential Model. My question is, can my Model have number of units greater than the number of input features?\nAt the moment, my features or csv columns are 118 and Model summary is as:Now, I'm getting 100% accuracy on both training and testing data, which is surprising to me. For 50 features, the model was giving 94.25% accuracy on training and 95.68% accuracy on testing data. Accuracy can be even 99 or 99.XX% but 100% is something wrong I guess? If it's 100% on test data, it's probably not the overfitting..? You can try \"Border Pairs Method\", \"Bipropagation\" or \"One step method\" to finding out how many neurons you need in each layer.", "answer_body": "0$\\begingroup$You can try \"Border Pairs Method\", \"Bipropagation\" or \"One step method\" to finding out how many neurons you need in each layer.https://www.researchgate.net/publication/322617800_New_Deep_Learning_Algorithms_beyond_Backpropagation_IBM_Developers_UnConference_2018_Zurich﻿ShareImprove this answerFollowanswered Dec 12 '20 at 8:54Bojan PlojBojan Ploj1$\\endgroup$1$\\begingroup$One line and link only answers often get flagged for deletion. It is helpful to add more detail if possible.$\\endgroup$– EthanDec 12 '20 at 19:03add a comment|"}
{"url": "https://datascience.stackexchange.com/questions/77277/what-is-the-meaning-of-each-element-in-input-shape-of-conv1d-in-keras", "question_head": "What is the meaning of each element in input_shape of Conv1D in Keras?", "question_body": "I have a time-series data for 3 classes (each class is 35 second) as I extract each 1 second for 95 feature extracting so my final data has shape (105,95) (rows for time and column for feature).I am currently building a 1D-CNN model for classification 3 class. But I got stuck as first layer of Conv1D. I have learnt that the input_shape of Convd1D is (batch_size, new_step, input_dim) but And one more question, I know that CNN required fixed input size. But I split my data into train and validation data as k parameter = 5, mean that my data will become: Train = (84,95) and Validation = (21,95). So with the same model how can we train and validate data with difference size?I just start to learn about CNN but still not know how Conv1D operate?I am appreciate your help.Thank you very much.Hear is my code (Which still get error because of first Conv1D input_shape): - This is the number of instances. Let's say it is M\n - This is the shape of each  instance. Each instance is (1 x N)\nSince input instances are of 1-D, the  input data become m x N. Had it been 2-D, it would have been m x N x NSize means the size of each instance which is same in this case i.e (1,95). Just the number of instances are different i.e. 21 in Test and 84 in Train.TimeSeries data -\n In TS learning, our initial data is just a time-based sequence of individual datapoint. We then make it Feature and Target by using a certain points (e.g. M) as Feature and  (M+1)th as Target. M is identified based on TS characteristics.\n\nThese are 3 train records.", "answer_body": "1$\\begingroup$X[train].shape[0] - This is the number of instances. Let's say it is MX[train].shape[1] - This is the shape of each  instance. Each instance is (1 x N)Since input instances are of 1-D, the  input data become m x N. Had it been 2-D, it would have been m x Nx x NyAnd one more question, I know that CNN required fixed input size. But I split my data into train and validation data as k parameter = 5, mean that my data will become: Train = (84,95) and Validation = (21,95). So with the same model how can we train and validate data with difference sizeSize means the size of each instance which is same in this case i.e (1,95). Just the number of instances are different i.e. 21 in Test and 84 in Train.TimeSeries data -In TS learning, our initial data is just a time-based sequence of individual datapoint. We then make it Feature and Target by using a certain points (e.g. M) as Feature and  (M+1)th as Target. M is identified based on TS characteristics.e.g.1,2,3,4,5 will be (If I use 2 Feature)1,2(X) --> 3(Y)2,3(X) --> 4(Y)3,4(X) --> 5(Y) etc.These are 3 train records.May read this too - Machinelearningmastery﻿ShareImprove this answerFollowanswered Jul 7 '20 at 8:3410xAI10xAI3,51422 gold badges55 silver badges2020 bronze badges$\\endgroup$2$\\begingroup$First of all, thank you for answer my question. Say if I am wrong but I think that input=shape of Conv1D is a 2D shape matrix so that we can aplly filter on that input. Because of that I think the input shape should be (84,95) as Train data and (21,95) as Validation Data.$\\endgroup$– Q.H.ChuJul 7 '20 at 10:05$\\begingroup$Convolution will not happen across instances. It happens on instances individually. I assumed the problem as univariate. So, it will be a 1-D Image and 1-D convolution(For analogy). The kernel will slide in 1-D. Read the link. It has explained most of the stuff$\\endgroup$– 10xAIJul 7 '20 at 10:26add a comment|"}
